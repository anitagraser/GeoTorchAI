{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dee2483",
   "metadata": {},
   "source": [
    "# This Example shows the Prediction of Bike Flow in the NYC City using the deep learning model ST-ResNet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4f1cee6",
   "metadata": {},
   "source": [
    "Find the details of the ST-ResNet model in the <a href=\"https://dl.acm.org/doi/10.5555/3298239.3298479\">corresponding paper</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0b8ec05",
   "metadata": {},
   "source": [
    "Details of the dataset can be found <a href=\"https://github.com/FIBLAB/DeepSTN\">here</a>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6762142",
   "metadata": {},
   "source": [
    "### Import Modules and Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f799943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from geotorchai.models.grid import STResNet\n",
    "from geotorchai.datasets.grid import BikeNYCDeepSTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209e18ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Documents\\\\GitHub\\\\GeoTorchAI\\\\binders'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f29ba9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define parameters\n",
    "len_closeness = 3\n",
    "len_period = 4\n",
    "len_trend = 4\n",
    "nb_residual_unit = 4\n",
    "map_height, map_width = 21, 12\n",
    "nb_flow = 2\n",
    "nb_area = 81\n",
    "T = 24\n",
    "\n",
    "epoch_nums = 100\n",
    "learning_rate = 0.0002\n",
    "batch_size = 32\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "params = {'batch_size': batch_size, 'shuffle': False}\n",
    "\n",
    "## make sure that PATH_TO_DATASET exists in the running directory\n",
    "PATH_TO_DATASET = \"../data/deepstn\"\n",
    "MODEL_SAVE_PATH = \"model-stresnet\"\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_PATH,'model.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee46d96a",
   "metadata": {},
   "source": [
    "The spatial map size of the dataset is 21x12. \n",
    "\n",
    "flow_data.npy ( TimeLenth x In&OutFlow x MapHeight x MapWidth = 4392 x 2 x 21 x 12 ) \n",
    "\n",
    "poi_data.npy ( PoICategories x MapHeight x MapWidth = 9 x 21 x 12 ) \n",
    "\n",
    "https://github.com/FIBLAB/DeepSTN#datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e14f3737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4392\n",
      "2\n",
      "21\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "data = np.load(open(PATH_TO_DATASET + \"/flow_data.npy\", \"rb\"))\n",
    "print(len(data))\n",
    "print(len(data[0]))\n",
    "print(len(data[0][0]))\n",
    "print(len(data[0][0][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed609405",
   "metadata": {},
   "source": [
    "### Loading Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbc3cdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloading started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17708640/17708640 [00:00<00:00, 39952890.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloading finished\n",
      "File downloading started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18224/18224 [00:00<00:00, 9057589.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloading finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Load training and test dataset\n",
    "full_dataset = BikeNYCDeepSTN(root = PATH_TO_DATASET, download=True) \n",
    "\n",
    "## get the min-max-difference of normalized data for future use in calculating actual losses\n",
    "min_max_diff = full_dataset.get_min_max_difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62de05f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc6bc12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BikeNYCDeepSTN in module geotorchai.datasets.grid.nyc_bike_deepstn object:\n",
      "\n",
      "class BikeNYCDeepSTN(torch.utils.data.dataset.Dataset)\n",
      " |  BikeNYCDeepSTN(root, download=False, len_closeness=3, len_period=4, len_trend=4, T_closeness=1, T_period=24, T_trend=168, normalize=True)\n",
      " |  \n",
      " |  This dataset is based on https://github.com/FIBLAB/DeepSTN/tree/master/BikeNYC/DATA\n",
      " |  Grid map_height and map_width = 21 and 12\n",
      " |  \n",
      " |  Parameters\n",
      " |  ..........\n",
      " |  root (String) - Path to the dataset if it is already downloaded. If not downloaded, it will be downloaded in the given path.\n",
      " |  download (Boolean, Optional) - Set to True if dataset is not available in the given directory. Default: False\n",
      " |  is_training_data (Boolean, Optional) - Set to True if you want to create the training dataset, False for testing dataset. Default: True\n",
      " |  test_ratio (Float, Optional) - Length fraction of the test dataset. Default: 0.1\n",
      " |  len_closeness (Int, Optional) - Length of closeness. Default: 3\n",
      " |  len_period (Int, Optional) - Length of period. Default: 4\n",
      " |  len_trend (Int, Optional) - Length of trend. Default: 4\n",
      " |  T_closeness (Int, Optional) - Closeness length of T_data. Default: 1\n",
      " |  T_period (Int, Optional) - Period length of T_data. Default: 24\n",
      " |  T_trend (Int, Optional) - Trend length of T_data. Default: 24*7\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BikeNYCDeepSTN\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index: int)\n",
      " |  \n",
      " |  __init__(self, root, download=False, len_closeness=3, len_period=4, len_trend=4, T_closeness=1, T_period=24, T_trend=168, normalize=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  get_min_max_difference(self)\n",
      " |      ## This method returns the difference between maximum and minimum values of this dataset\n",
      " |  \n",
      " |  merge_closeness_period_trend(self, lead_time=48)\n",
      " |      Call this method if you want to iterate the dataset as a sequence of histories and predictions instead of closeness, period, and trend.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ..........\n",
      " |      history_length (Int) - Length of history data in sequence of each sample\n",
      " |      predict_length (Int) - Length of prediction data in sequence of each sample\n",
      " |  \n",
      " |  set_sequential_representation(self, history_length, prediction_length)\n",
      " |      Call this method if you want to iterate the dataset as a sequence of histories and predictions instead of closeness, period, and trend.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ..........\n",
      " |      history_length (Int) - Length of history data in sequence of each sample\n",
      " |      predict_length (Int) - Length of prediction data in sequence of each sample\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  DATA_URL = 'https://raw.githubusercontent.com/FIBLAB/DeepSTN/master/Bi...\n",
      " |  \n",
      " |  POI_URL = 'https://raw.githubusercontent.com/FIBLAB/DeepSTN/master/Bik...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(full_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78a06ab1",
   "metadata": {},
   "source": [
    "### Split Train Dataset into Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09fd64b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize training and validation indices to split the dataset\n",
    "dataset_size = len(full_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "val_split = int(np.floor((1 - (validation_ratio + test_ratio)) * dataset_size))\n",
    "test_split = int(np.floor((1 - test_ratio) * dataset_size))\n",
    "train_indices, val_indices, test_indices = indices[:val_split], indices[val_split:test_split], indices[test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e680ad99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3720"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f03105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define training and validation data sampler\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "## Define training and validation data loader\n",
    "train_loader = DataLoader(full_dataset, **params, sampler=train_sampler)\n",
    "val_loader = DataLoader(full_dataset, **params, sampler=valid_sampler)\n",
    "test_loader = DataLoader(full_dataset, **params, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ceb81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(train_loader):\n",
    "    X_c = sample[\"x_closeness\"].type(torch.FloatTensor)\n",
    "    X_p = sample[\"x_period\"].type(torch.FloatTensor)\n",
    "    X_t = sample[\"x_trend\"].type(torch.FloatTensor)\n",
    "    Y_batch = sample[\"y_data\"].type(torch.FloatTensor)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "190942dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "2\n",
      "21\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "x = sample[\"y_data\"]\n",
    "#torch.max(x)\n",
    "print(len(x[0]))\n",
    "print(len(x[0][0]))\n",
    "print(len(x[0][0][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c026f70",
   "metadata": {},
   "source": [
    "### Initialize Model and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b021059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set device to CPU or GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "## Define Model\n",
    "model = STResNet((len_closeness, nb_flow, map_height, map_width),\n",
    "                (len_period, nb_flow, map_height, map_width),\n",
    "                (len_trend, nb_flow , map_height, map_width),\n",
    "                external_dim = None, nb_residual_unit = nb_residual_unit)\n",
    "## Define hyper-parameters\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.to(device)\n",
    "loss_fn.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f93c22d3",
   "metadata": {},
   "source": [
    "### Method for Returning Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3745a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before starting training, define a method to calculate validation loss\n",
    "def get_validation_loss(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    mean_loss = []\n",
    "    for i, sample in enumerate(data_loader):\n",
    "        X_c = sample[\"x_closeness\"].type(torch.FloatTensor).to(device)\n",
    "        X_p = sample[\"x_period\"].type(torch.FloatTensor).to(device)\n",
    "        X_t = sample[\"x_trend\"].type(torch.FloatTensor).to(device)\n",
    "        Y_batch = sample[\"y_data\"].type(torch.FloatTensor).to(device)\n",
    "\n",
    "        outputs = model(X_c, X_p, X_t)\n",
    "        mse = criterion(outputs, Y_batch).item()\n",
    "        mean_loss.append(mse)\n",
    "\n",
    "    mean_loss = np.mean(mean_loss)\n",
    "    return mean_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "732ea6c3",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "897e8df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 0.0023\n",
      "Mean validation loss: 0.003763918900707116\n",
      "Best model saved!\n",
      "Epoch [2/100], Training Loss: 0.0023\n",
      "Mean validation loss: 0.002776783367153257\n",
      "Best model saved!\n",
      "Epoch [3/100], Training Loss: 0.0014\n",
      "Mean validation loss: 0.002140190830687061\n",
      "Best model saved!\n",
      "Epoch [4/100], Training Loss: 0.0015\n",
      "Mean validation loss: 0.0018404972894738119\n",
      "Best model saved!\n",
      "Epoch [5/100], Training Loss: 0.0011\n",
      "Mean validation loss: 0.0016444134525954723\n",
      "Best model saved!\n",
      "Epoch [6/100], Training Loss: 0.0010\n",
      "Mean validation loss: 0.0014739853892630588\n",
      "Best model saved!\n",
      "Epoch [7/100], Training Loss: 0.0012\n",
      "Mean validation loss: 0.0013732275692746043\n",
      "Best model saved!\n",
      "Epoch [8/100], Training Loss: 0.0007\n",
      "Mean validation loss: 0.0011972895639094834\n",
      "Best model saved!\n",
      "Epoch [9/100], Training Loss: 0.0005\n",
      "Mean validation loss: 0.0012004761083517224\n",
      "Epoch [10/100], Training Loss: 0.0008\n",
      "Mean validation loss: 0.0011059921962441877\n",
      "Best model saved!\n",
      "Epoch [11/100], Training Loss: 0.0007\n",
      "Mean validation loss: 0.0011547086420857038\n",
      "Epoch [12/100], Training Loss: 0.0007\n",
      "Mean validation loss: 0.0010651670454535633\n",
      "Best model saved!\n",
      "Epoch [13/100], Training Loss: 0.0007\n",
      "Mean validation loss: 0.0010436804780814175\n",
      "Best model saved!\n",
      "Epoch [14/100], Training Loss: 0.0006\n",
      "Mean validation loss: 0.0009786135342437774\n",
      "Best model saved!\n",
      "Epoch [15/100], Training Loss: 0.0006\n",
      "Mean validation loss: 0.0009181309190656369\n",
      "Best model saved!\n",
      "Epoch [16/100], Training Loss: 0.0006\n",
      "Mean validation loss: 0.000920480068695421\n",
      "Epoch [17/100], Training Loss: 0.0005\n",
      "Mean validation loss: 0.0008747847411238278\n",
      "Best model saved!\n",
      "Epoch [18/100], Training Loss: 0.0004\n",
      "Mean validation loss: 0.0008387828177850073\n",
      "Best model saved!\n",
      "Epoch [19/100], Training Loss: 0.0004\n",
      "Mean validation loss: 0.0008411253753971929\n",
      "Epoch [20/100], Training Loss: 0.0005\n",
      "Mean validation loss: 0.0008059426375742381\n",
      "Best model saved!\n",
      "Epoch [21/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0007897876882149527\n",
      "Best model saved!\n",
      "Epoch [22/100], Training Loss: 0.0005\n",
      "Mean validation loss: 0.0007913701119832695\n",
      "Epoch [23/100], Training Loss: 0.0004\n",
      "Mean validation loss: 0.0007439527835231274\n",
      "Best model saved!\n",
      "Epoch [24/100], Training Loss: 0.0004\n",
      "Mean validation loss: 0.0007482936974459639\n",
      "Epoch [25/100], Training Loss: 0.0004\n",
      "Mean validation loss: 0.000740593473892659\n",
      "Best model saved!\n",
      "Epoch [26/100], Training Loss: 0.0004\n",
      "Mean validation loss: 0.0007628723881983509\n",
      "Epoch [27/100], Training Loss: 0.0005\n",
      "Mean validation loss: 0.0007555552777679017\n",
      "Epoch [28/100], Training Loss: 0.0004\n",
      "Mean validation loss: 0.0007165328812940667\n",
      "Best model saved!\n",
      "Epoch [29/100], Training Loss: 0.0004\n",
      "Mean validation loss: 0.0007059506994361678\n",
      "Best model saved!\n",
      "Epoch [30/100], Training Loss: 0.0005\n",
      "Mean validation loss: 0.0007024714917254945\n",
      "Best model saved!\n",
      "Epoch [31/100], Training Loss: 0.0004\n",
      "Mean validation loss: 0.0006893085956107825\n",
      "Best model saved!\n",
      "Epoch [32/100], Training Loss: 0.0005\n",
      "Mean validation loss: 0.000699153977620881\n",
      "Epoch [33/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0007075791072566062\n",
      "Epoch [34/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0006611731417554741\n",
      "Best model saved!\n",
      "Epoch [35/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0006726944654171044\n",
      "Epoch [36/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0006658901911578141\n",
      "Epoch [37/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0006508715523523279\n",
      "Best model saved!\n",
      "Epoch [38/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0006287226885130318\n",
      "Best model saved!\n",
      "Epoch [39/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0006319763924693689\n",
      "Epoch [40/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0006174562950036488\n",
      "Best model saved!\n",
      "Epoch [41/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0006372735442710109\n",
      "Epoch [42/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0006380460035870783\n",
      "Epoch [43/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0006263581235543825\n",
      "Epoch [44/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0005812130475533195\n",
      "Best model saved!\n",
      "Epoch [45/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0006326778263125258\n",
      "Epoch [46/100], Training Loss: 0.0004\n",
      "Mean validation loss: 0.0005956946212487916\n",
      "Epoch [47/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0005584923079974639\n",
      "Best model saved!\n",
      "Epoch [48/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0006106266470548386\n",
      "Epoch [49/100], Training Loss: 0.0004\n",
      "Mean validation loss: 0.0005834920884808525\n",
      "Epoch [50/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0006265146997369205\n",
      "Epoch [51/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0005703733792567315\n",
      "Epoch [52/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005710733894375153\n",
      "Epoch [53/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0005647528523695655\n",
      "Epoch [54/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0005696419878707578\n",
      "Epoch [55/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0005593384000045868\n",
      "Epoch [56/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.000566137743589934\n",
      "Epoch [57/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005748516826618774\n",
      "Epoch [58/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0005401552843977697\n",
      "Best model saved!\n",
      "Epoch [59/100], Training Loss: 0.0004\n",
      "Mean validation loss: 0.000550800708879251\n",
      "Epoch [60/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005595697827326754\n",
      "Epoch [61/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005343035494054978\n",
      "Best model saved!\n",
      "Epoch [62/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005241579783614725\n",
      "Best model saved!\n",
      "Epoch [63/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005666702951809081\n",
      "Epoch [64/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0005656192515743896\n",
      "Epoch [65/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005155662123191481\n",
      "Best model saved!\n",
      "Epoch [66/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005216539890777009\n",
      "Epoch [67/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0005385732098754185\n",
      "Epoch [68/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005018210673976379\n",
      "Best model saved!\n",
      "Epoch [69/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005034492884684975\n",
      "Epoch [70/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005630433127710907\n",
      "Epoch [71/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0006202941998102082\n",
      "Epoch [72/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005705893248280821\n",
      "Epoch [73/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0005024209507003737\n",
      "Epoch [74/100], Training Loss: 0.0003\n",
      "Mean validation loss: 0.0005167530956290042\n",
      "Epoch [75/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005421325428566585\n",
      "Epoch [76/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005200764741554545\n",
      "Epoch [77/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0004786427428674263\n",
      "Best model saved!\n",
      "Epoch [78/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0004737732621530692\n",
      "Best model saved!\n",
      "Epoch [79/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0004906361282337457\n",
      "Epoch [80/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0004928490598103963\n",
      "Epoch [81/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.00048794907100576285\n",
      "Epoch [82/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005023273794601361\n",
      "Epoch [83/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0004841398294956889\n",
      "Epoch [84/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0005027557684419056\n",
      "Epoch [85/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.00048289120362217847\n",
      "Epoch [86/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.000493564754530477\n",
      "Epoch [87/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.00047951880454396206\n",
      "Epoch [88/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.000501192075413807\n",
      "Epoch [89/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0004987333910927797\n",
      "Epoch [90/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0004969388295042639\n",
      "Epoch [91/100], Training Loss: 0.0001\n",
      "Mean validation loss: 0.0005147622408306537\n",
      "Epoch [92/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0004626682057278231\n",
      "Best model saved!\n",
      "Epoch [93/100], Training Loss: 0.0001\n",
      "Mean validation loss: 0.0004983157365738103\n",
      "Epoch [94/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.00046035891136853024\n",
      "Best model saved!\n",
      "Epoch [95/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.00046053882882309455\n",
      "Epoch [96/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.00047846863890299574\n",
      "Epoch [97/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.00048064310491705936\n",
      "Epoch [98/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0004549008178097817\n",
      "Best model saved!\n",
      "Epoch [99/100], Training Loss: 0.0002\n",
      "Mean validation loss: 0.0004610073519870639\n",
      "Epoch [100/100], Training Loss: 0.0001\n",
      "Mean validation loss: 0.00044548241567099467\n",
      "Best model saved!\n"
     ]
    }
   ],
   "source": [
    "## Perform training and validation\n",
    "min_val_loss = None\n",
    "for e in range(epoch_nums):\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        X_c = sample[\"x_closeness\"].type(torch.FloatTensor).to(device)\n",
    "        X_p = sample[\"x_period\"].type(torch.FloatTensor).to(device)\n",
    "        X_t = sample[\"x_trend\"].type(torch.FloatTensor).to(device)\n",
    "        Y_batch = sample[\"y_data\"].type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_c, X_p, X_t)\n",
    "        loss = loss_fn(outputs, Y_batch)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Training Loss: {:.4f}'.format(e + 1, epoch_nums, loss.item()))\n",
    "\n",
    "    ## Perform model validation after finishing each epoch training\n",
    "    val_loss = get_validation_loss(model, val_loader, loss_fn, device)\n",
    "    print('Mean validation loss:', val_loss)\n",
    "\n",
    "    if min_val_loss == None or val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print('Best model saved!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57c4499f",
   "metadata": {},
   "source": [
    "### Define a Method to Return MSE, MAE, RMSE Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa2cbb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before testing, Define a method to calculate three types of loss: MSE, MAE, RMSE\n",
    "def compute_errors(preds, y_true):\n",
    "    pred_mean = preds[:, 0:2]\n",
    "    diff = y_true - pred_mean\n",
    "\n",
    "    mse = np.mean(diff ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(diff))\n",
    "\n",
    "    return mse, mae, rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b6d3503",
   "metadata": {},
   "source": [
    "### Evaluate on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4665a4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mse: 0.000433 mae: 0.008000 rmse (norm): 0.020685, mae (real): 2.947979, rmse (real): 7.622255\n"
     ]
    }
   ],
   "source": [
    "## Perform testing on the best model with test dataset\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=lambda storage, loc: storage))\n",
    "\n",
    "rmse_list=[]\n",
    "mse_list=[]\n",
    "mae_list=[]\n",
    "for i, sample in enumerate(test_loader):\n",
    "    X_c = sample[\"x_closeness\"].type(torch.FloatTensor).to(device)\n",
    "    X_p = sample[\"x_period\"].type(torch.FloatTensor).to(device)\n",
    "    X_t = sample[\"x_trend\"].type(torch.FloatTensor).to(device)\n",
    "    Y_batch = sample[\"y_data\"].type(torch.FloatTensor).to(device)\n",
    "\n",
    "    outputs = model(X_c, X_p, X_t)\n",
    "    mse, mae, rmse = compute_errors(outputs.cpu().data.numpy(), Y_batch.cpu().data.numpy())\n",
    "\n",
    "    rmse_list.append(rmse)\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "    \n",
    "rmse = np.mean(rmse_list)\n",
    "mse = np.mean(mse_list)\n",
    "mae = np.mean(mae_list)\n",
    "\n",
    "print('Test mse: %.6f mae: %.6f rmse (norm): %.6f, mae (real): %.6f, rmse (real): %.6f' % (mse, mae, rmse, mae * min_max_diff/2, rmse*min_max_diff/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96c1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
